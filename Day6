Model Used

Decision Tree Classifier

Non-linear supervised classification model

Handles both numerical and categorical (encoded) features

ðŸ”¹ Why Decision Trees

Logistic Regression assumes linear relationships

Decision Trees capture non-linear patterns

No need for feature scaling

Easy to interpret compared to many ML models

ðŸ”¹ Baseline Comparison

Logistic Regression used as baseline

Metrics compared:

Accuracy

F1-score

Decision Tree showed improved flexibility

Higher risk of overfitting compared to Logistic Regression

ðŸ”¹ Overfitting Observation

Default Decision Tree tends to overfit

Very deep trees memorize training data

Symptoms:

High training accuracy

Lower test performance

ðŸ”¹ Depth Control (max_depth)

max_depth = 3 â†’ underfitting

max_depth = 5 â†’ balanced performance

max_depth = 10 â†’ overfitting tendency

Optimal depth improves generalization

ðŸ”¹ Feature Importance

Extracted using feature_importances_

Shows contribution of each feature to predictions

Importance values sum to 1

Top features influenced survival prediction

ðŸ”¹ Key Important Features (Titanic)

Sex

Fare

Passenger Class (Pclass)

FamilySize

IsAlone

Results aligned with EDA insights from Day 3

ðŸ”¹ Decision Tree Strengths

Captures non-linear relationships

Interpretable feature importance

Works well with mixed feature types

No scaling required

ðŸ”¹ Decision Tree Limitations

Prone to overfitting

Sensitive to small data changes

Single tree is unstable

Not ideal for high-variance data

ðŸ”¹ Final Verdict

Decision Tree outperforms Logistic Regression in flexibility

Requires tuning to avoid overfitting

Best used as a base learner

Ensemble methods needed for robustness
