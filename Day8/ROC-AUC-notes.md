ðŸ”¹ Why Model Comparison is Needed

Single accuracy score is misleading

Train-test split may be biased

Different models behave differently on same data

Fair evaluation is required before deployment

ðŸ”¹ Models Compared

Logistic Regression

Decision Tree

Random Forest

Same dataset and same split used for fairness

ðŸ”¹ Limitations of Accuracy

Does not handle class imbalance well

Ignores confidence of predictions

Two models can have same accuracy but different behavior

ðŸ”¹ ROC Curve

Plots True Positive Rate vs False Positive Rate

Shows trade-off between sensitivity and specificity

Visual comparison of model performance

ðŸ”¹ ROC-AUC Score

Measures ability to separate classes

Value ranges from 0 to 1

0.5 â†’ random guessing

Closer to 1 â†’ better model

Threshold-independent metric

ðŸ”¹ ROC-AUC Observations

Logistic Regression performed reasonably

Decision Tree showed unstable behavior

Random Forest achieved highest ROC-AUC

Ensemble model handled class separation best

ðŸ”¹ Cross-Validation Concept

Splits data into multiple folds

Trains and tests model multiple times

Reduces dependency on a single split

Provides reliable performance estimate

ðŸ”¹ Cross-Validation Results

Evaluated using F1-score

Mean score indicates average performance

Standard deviation indicates stability

Lower std â†’ more stable model

ðŸ”¹ Model Stability Analysis

Logistic Regression â†’ stable but limited

Decision Tree â†’ high variance

Random Forest â†’ best balance of bias and variance

ðŸ”¹ Final Model Selection

Random Forest selected as final model

Highest ROC-AUC score

Consistent cross-validation performance

Suitable for structured tabular data

ðŸ”¹ Key Takeaways

Always compare multiple models

Use ROC-AUC instead of accuracy alone

Cross-validation is essential

Ensemble models often perform better
